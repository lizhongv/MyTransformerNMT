{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38737e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import copy\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "# torch中变量封装函数Variable.\n",
    "# 扩展资料: https://www.jb51.net/article/177996.htm\n",
    "# 源码来自：https://github.com/harvardnlp/annotated-transformer/blob/master/AnnotatedTransformer.ipynb\n",
    "# 解释来自：https://blog.csdn.net/m0_56192771/article/details/118087175        https://blog.csdn.net/JamesX666/article/details/126454270\n",
    "# 知乎解释： https://zhuanlan.zhihu.com/p/398039366"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e94c31c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m x\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m],[\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m]])\n\u001b[1;32m----> 2\u001b[0m var \u001b[38;5;241m=\u001b[39m Variable(\u001b[43mtensor\u001b[49m, requires_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# tensor不能反向传播，variable可以反向传播。\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# y = torch.mean(x)\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# y.backward()  ERROR\u001b[39;00m\n\u001b[0;32m      5\u001b[0m y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmean(variable)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tensor' is not defined"
     ]
    }
   ],
   "source": [
    "x= torch.FloatTensor([[1,2],[3,4]])\n",
    "var = Variable(tensor, requires_grad=True) # tensor不能反向传播，variable可以反向传播。\n",
    "# y = torch.mean(x)\n",
    "# y.backward()  ERROR\n",
    "y = torch.mean(variable)\n",
    "y.backward()\n",
    "print(var.grad, var.data)  # var.data 转化为tensor形式\n",
    "\n",
    "\n",
    "# requires_grad: True tensor需要计算梯度\n",
    "# grad_fn：记录tensor如何得到，方便计算梯度\n",
    "# grad：backward() 后可以查看tensor的梯度\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = x + 2\n",
    "# y.backward()  # RuntimeError: grad can be implicitly created only for scalar outputs  \n",
    "# y不是一个标量，若scalar对tensor求导直接backward()；若tensor对tensor求导，先求出Jacobian矩阵中每个元素的梯度值，然后将这个Jacobian矩阵与grad_tensors参数对应的矩阵进行对应点乘\n",
    "## grad_tensors=None\n",
    "## retain_graph=None\n",
    "## create_graph=False\n",
    "## grad_variables=None\n",
    "y.backward(torch.ones_like(y))\n",
    "print(x.grad)\n",
    "\n",
    "z = y*4\n",
    "loss1 = z.mean()\n",
    "loss2 = z.sum()\n",
    "print(loss1, loss2)\n",
    "loss1.backward() # 加上 retain_graph 保存backward（）后的中间参数，否则执行完backward（）后中间参数全部释放掉了，影响下一次的backward（）\n",
    "print(loss1, loss2)\n",
    "print(x.grad)\n",
    "# loss2.backward() # RuntimeError: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f9928c",
   "metadata": {},
   "source": [
    "# 一、输入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c4d88",
   "metadata": {},
   "source": [
    "## 1.1 词嵌入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76f54529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param vocab: 词表大小\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x): \n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "#         w = self.lut.weight\n",
    "#         print(w)\n",
    "#         print(w.shape)\n",
    "        return self.lut(x) * math.sqrt(self.d_model)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "613f23b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[ -6.3135,  16.6665, -26.1533,  ..., -26.1079,  16.0799,  49.0220],\n",
       "          [ -1.6383, -23.3749, -34.3804,  ...,  -3.0579,   1.4318, -41.3992],\n",
       "          [ -9.6149,  32.7269,   9.7242,  ...,  36.6066,   7.1971,  25.6940],\n",
       "          [ 13.2073,  -7.5062,  12.6934,  ..., -23.6169,  32.8903, -26.9443]],\n",
       " \n",
       "         [[ -3.7118,   6.7728,  -2.1114,  ..., -14.4089, -34.6631,   1.5404],\n",
       "          [-15.5792, -12.9442,  -0.7474,  ...,   9.8891, -15.4912,  19.8836],\n",
       "          [-41.0385,  22.0167, -30.9122,  ..., -32.6049,   6.3138,  11.4358],\n",
       "          [-45.0965,  15.8813, -11.8618,  ...,  20.6125, -28.9372, -14.4448]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512 \n",
    "vocab = 1000  \n",
    "\n",
    "x = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))  # [2, 4] [batch, seq_len]\n",
    "\n",
    "# 模型实例化\n",
    "emb = Embeddings(d_model, vocab)  # padding_idx=\n",
    "\n",
    "# 输入输出\n",
    "out_emb = emb(x)\n",
    "\n",
    "out_emb.shape, out_emb # [2, 4, 512]  [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ba81b",
   "metadata": {},
   "source": [
    "## 1.2 位置编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "114a9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout, max_len=5000):\n",
    "        \"\"\"\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: 置0比率，让部分神经元失效\n",
    "        :param max_len: 语料库中最长句子的长度\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model) # [max_len, d_model]\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # [0~max_len-1, 1]\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)    # [1, max_len, d_model] \n",
    "        self.register_buffer(\"pe\", pe)\n",
    "\n",
    "    def forward(self, x):  \n",
    "        \"\"\"\n",
    "        :param x:  [batch, seq_len, d_model]\n",
    "        :return: [batch, seq_len, d_model]    max_len—>seq_len\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)  # 默认 max_len=5000，适配 pe.size(1) == x.size(1)，不需要梯度计算\n",
    "        return self.dropout(x) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e4f134b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[ -7.0150,  19.6294, -29.0592,  ..., -27.8977,  17.8666,  55.5800],\n",
       "          [ -0.8853, -25.3718, -37.2873,  ...,  -0.0000,   1.5910, -44.8880],\n",
       "          [ -9.6729,  35.9008,  11.8451,  ...,  41.7851,   7.9970,  29.6600],\n",
       "          [ 14.8316,  -9.4402,   0.0000,  ...,  -0.0000,  36.5452, -28.8270]],\n",
       " \n",
       "         [[ -4.1242,   8.6364,  -2.3460,  ..., -14.8987, -38.5145,   2.8227],\n",
       "          [-16.3753, -13.7821,   0.0827,  ...,  12.0990, -17.2123,  23.2040],\n",
       "          [-44.5880,  24.0006, -33.3065,  ..., -35.1165,   7.0156,  13.8175],\n",
       "          [ -0.0000,  16.5459, -12.9075,  ...,  24.0139, -32.1521, -14.9386]]],\n",
       "        grad_fn=<MulBackward0>))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "dropout = 0.1  # 置0比率为0.1\n",
    "\n",
    "# 模型实例化\n",
    "pe = PositionalEncoding(d_model, dropout)\n",
    "\n",
    "# 输入输出\n",
    "out_pe = pe(out_emb)\n",
    "\n",
    "out_pe.shape, out_pe  # [2, 4, 512] [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4c1156a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 绘制向量中特征的分布曲线         内核直接挂掉\n",
    "# plt.figure(figsize=(15, 5))  # 创建一张 15*5 大小的画布\n",
    "# pe = PositionalEncoding(20, 0)  # 词嵌入维度20， dropout=0\n",
    "# y = pe(Variable(torch.zeros(1, 100, 20)))  # [batch, seq_len, d_model]\n",
    "# plt.plot(np.arange(100), y[0, :, 4:8].data.numpy())\n",
    "# plt.legend([\"dim %d\" % p for p in [4, 5, 6, 7]])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5bb51c",
   "metadata": {},
   "source": [
    "# 二、注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846d447",
   "metadata": {},
   "source": [
    "## 2.1 掩码张量 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d907061",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    \"\"\"\"\n",
    "    :param size: 掩码张量最后两维度大小（方阵）\n",
    "    :return: [1, size, size]\n",
    "    \"\"\"\n",
    "    \"Mask out subsequent positions.\"\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(\n",
    "        torch.uint8\n",
    "    )\n",
    "     # torch.triu 返回上三角阵，diagonal=1 对角线不是掩码\n",
    "    return subsequent_mask == 0  # 下三角矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9f4746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 5, 5]),\n",
       " tensor([[[ True, False, False, False, False],\n",
       "          [ True,  True, False, False, False],\n",
       "          [ True,  True,  True, False, False],\n",
       "          [ True,  True,  True,  True, False],\n",
       "          [ True,  True,  True,  True,  True]]]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 5 \n",
    "\n",
    "# 函数输入输出\n",
    "sm = subsequent_mask(size) # [1, size, size]\n",
    "sm.shape, sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "069d83d3",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mimshow(subsequent_mask(\u001b[38;5;241m20\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# plt.figure(figsize=(5, 5))\n",
    "# plt.imshow(subsequent_mask(20)[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275dcef5",
   "metadata": {},
   "source": [
    "## 2.2 注意力机制 函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d9487318",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Compute 'Scaled Dot Product Attention'\"\n",
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    \"\"\"\n",
    "    :param query: [batch, seq_len, d_model]\n",
    "    :param key: [batch, seq_len, d_model]\n",
    "    :param value: [batch, seq_len, d_model]\n",
    "    :param mask: \n",
    "    :param dropout: \n",
    "    :return: [batch, seq_len, seq_len]*[batch, seq_len, d_model]=[batch, seq_len, d_model]  [batch, seq_len, seq_len]\n",
    "    \"\"\"\n",
    "    d_k = query.size(-1)  # d_model 一般为词嵌入维度\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(\n",
    "        d_k)  # [batch, seq_len, d_model]*[batch, d_model, seq_len] = [batch, seq_len, seq_len]\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)  # 0 处填充-1e9（-inf）, mask采用广播方式\n",
    "#     print(\"scores:\", scores.shape, scores)\n",
    "    \n",
    "    p_attn = scores.softmax(dim=-1) # [batch, seq_len, seq_len] 每个词语其他词的关系度\n",
    "    \n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn) \n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "625fef22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4]) tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 4, 512]) tensor([[[ -7.0150,  19.6294, -29.0592,  ..., -27.8977,  17.8666,  55.5800],\n",
      "         [ -0.8853, -25.3718, -37.2873,  ...,   0.0000,   1.5910, -44.8880],\n",
      "         [ -9.6729,  35.9008,  11.8451,  ...,  41.7851,   7.9970,  29.6600],\n",
      "         [ 14.8316,  -9.4402,   0.0000,  ...,   0.0000,  36.5452, -28.8270]],\n",
      "\n",
      "        [[ -4.1242,   8.6364,  -2.3460,  ..., -14.8987, -38.5145,   2.8227],\n",
      "         [-16.3753, -13.7821,   0.0827,  ...,  12.0990, -17.2123,  23.2040],\n",
      "         [-44.5880,  24.0006, -33.3065,  ..., -35.1165,   7.0156,  13.8175],\n",
      "         [  0.0000,  16.5459, -12.9075,  ...,  24.0139, -32.1521, -14.9386]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 无掩码\n",
    "query = key = value = out_pe\n",
    "attn, p_attn = attention(query, key, value) \n",
    "print(p_attn.shape, p_attn)  \n",
    "print(attn.shape, attn)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a5dae3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 4]) tensor([[[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]],\n",
      "\n",
      "        [[1., 0., 0., 0.],\n",
      "         [0., 1., 0., 0.],\n",
      "         [0., 0., 1., 0.],\n",
      "         [0., 0., 0., 1.]]], grad_fn=<SoftmaxBackward0>)\n",
      "torch.Size([2, 4, 512]) tensor([[[ -7.0150,  19.6294, -29.0592,  ..., -27.8977,  17.8666,  55.5800],\n",
      "         [ -0.8853, -25.3718, -37.2873,  ...,   0.0000,   1.5910, -44.8880],\n",
      "         [ -9.6729,  35.9008,  11.8451,  ...,  41.7851,   7.9970,  29.6600],\n",
      "         [ 14.8316,  -9.4402,   0.0000,  ...,   0.0000,  36.5452, -28.8270]],\n",
      "\n",
      "        [[ -4.1242,   8.6364,  -2.3460,  ..., -14.8987, -38.5145,   2.8227],\n",
      "         [-16.3753, -13.7821,   0.0827,  ...,  12.0990, -17.2123,  23.2040],\n",
      "         [-44.5880,  24.0006, -33.3065,  ..., -35.1165,   7.0156,  13.8175],\n",
      "         [  0.0000,  16.5459, -12.9075,  ...,  24.0139, -32.1521, -14.9386]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 有/无 掩码\n",
    "query = key = value = out_pe  # [2,4,512]  [batch, seq_len, d_model]\n",
    "\n",
    "# mask = subsequent_mask(4) # [1,4,4]  [1, seq_len, seq_len]\n",
    "# print(mask.shape, mask)\n",
    "mask = None\n",
    "\n",
    "attn, p_attn = attention(query, key, value, mask=mask)\n",
    "print(p_attn.shape, p_attn) # [2,4,4]   [batch, seq_len, seq_len]\n",
    "print(attn.shape, attn)   # [2,4,512]   [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304ec971",
   "metadata": {},
   "source": [
    "## 2.3 多头注意力机制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "55c1950e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 克隆函数\n",
    "def clones(module, N):\n",
    "    \"\"\"\n",
    "    :param module:\n",
    "    :param N:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)]) # 对model进行N次深度拷贝        \n",
    "# nn.Sequential, nn.ModuleList "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "37bf986d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param h: 头数\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param dropout: 置0率\n",
    "        \"\"\"\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0  \n",
    "        self.d_k = d_model // h  # 每个头获得等量的词特征\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4) # 输入和输出维数相同\n",
    "        # 4个线性层，Q，K，V个需要一个，最后concat需要一个\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        :param query: [batch, seq_len, d_model]\n",
    "        :param key: [batch, seq_len, d_model]\n",
    "        :param value: [batch, seq_len, d_model]\n",
    "        :param mask: [head, seq_len, seq_len]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        if mask is not None:\n",
    "            # Same mask applied to all h heads.\n",
    "            mask = mask.unsqueeze(0)  # [head, 1, seq_len, seq_len]\n",
    "        nbatches = query.size(0)  # batch_size\n",
    "\n",
    "        # 1) Do all the linear projections in batch from d_model => h x d_k\n",
    "        query, key, value = [\n",
    "            lin(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for lin, x in zip(self.linears, (query, key, value))\n",
    "        ] # [batch, h, seq_len, d_k]\n",
    "\n",
    "        # 2) Apply attention on all the projected vectors in batch.\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "#         print(x.shape, self.attn.shape)  # [batch, h, seq_len, d_k]   [batch, h, seq_len, seq_len]\n",
    "\n",
    "        # 3) \"Concat\" using a view and apply a final linear.\n",
    "        x = x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k) # [batch, seq_len, d_model]\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "        return self.linears[-1](x)  # 最后经过一个线性层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "80ef9e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[  0.1201,  -6.7654,   0.5670,  ...,   6.3632,  -1.5364,   4.1432],\n",
       "          [ -1.9374, -13.7956,  18.1052,  ..., -23.3191,  10.7850, -10.7807],\n",
       "          [ 13.7793,  -6.2434,   2.9834,  ...,  -1.1365,  -2.2245,  -0.5206],\n",
       "          [ -2.5482,   3.5856,   8.4269,  ...,  -0.2525,  -2.3977,   3.4267]],\n",
       " \n",
       "         [[  3.0670,   7.3750, -10.7303,  ...,  -4.7194,  -4.7140, -10.2006],\n",
       "          [ -1.4688,   3.3906,  24.8131,  ...,  22.3113,   2.1377,  21.2604],\n",
       "          [  5.9435,  -5.2225,  -8.5219,  ...,  -9.9623,  -4.8698,  -4.9577],\n",
       "          [  8.9981,  -6.1869,   7.4167,  ...,  -3.9225, -10.8719,  -9.9060]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 有/无 掩码\n",
    "head = 8\n",
    "d_model = 512\n",
    "dropout = 0.2\n",
    "\n",
    "query = value = key = out_pe  # [2, 4, 512] => [batch, seq_len, d_model]\n",
    "\n",
    "# mask = subsequent_mask(4)  # [1, seq_len, seq_len]\n",
    "mask = None\n",
    "\n",
    "# 模型实例化\n",
    "mha = MultiHeadedAttention(head, d_model, dropout)\n",
    "\n",
    "# 输入输出\n",
    "out_mha = mha(query, key, value, mask)\n",
    "out_mha.shape, out_mha  # [2, 4, 512] [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69ab2f",
   "metadata": {},
   "source": [
    "# 三、 前馈全连接层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b0f19f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        :param d_model: 线性层的输入维度\n",
    "        :param d_ff: 线性层的输出维度\n",
    "        :param dropout:\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len, d_model]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        return self.w_2(self.dropout(F.relu(self.w_1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d6349d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[-2.2931,  3.1031,  6.6184,  ...,  0.0638,  1.8999,  2.3175],\n",
       "          [ 0.8127, -0.1430,  0.1979,  ...,  1.8851, -1.7223,  1.4125],\n",
       "          [-4.8497, -0.8918,  1.1326,  ..., -1.7741, -0.4349,  2.6070],\n",
       "          [-1.9955, -0.6763,  3.5122,  ..., -0.7023,  0.6840, -0.3097]],\n",
       " \n",
       "         [[-0.7978, -0.2064,  3.0988,  ...,  2.5182,  0.2775, -3.0580],\n",
       "          [ 3.0043, -0.2703, -0.8180,  ...,  3.1509,  0.5949,  1.4028],\n",
       "          [ 3.0991, -2.1678,  2.5744,  ...,  1.0916,  2.3163, -0.4694],\n",
       "          [ 0.4183, -2.4270, -1.1791,  ..., -2.3256, -5.4808, -0.0842]]],\n",
       "        grad_fn=<ViewBackward0>))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "d_ff = 64\n",
    "dropput = 0.2\n",
    "\n",
    "# 模型实例化\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "#  输入输出\n",
    "out_ff = ff(out_mha)\n",
    "\n",
    "out_ff.shape, out_ff  # [2, 4, 512] => [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8fd6a",
   "metadata": {},
   "source": [
    "# 四、子层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b91129",
   "metadata": {},
   "source": [
    "## 4.1 规范化层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b43fdc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"Construct a layernorm module (See citation for details).\"\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        \"\"\"\n",
    "        :param features: 词嵌入维度\n",
    "        :param eps: 足够小的数，防止分母为0\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len, d_model]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d44d1bf2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[-9.3551e-01,  1.2052e+00,  2.5997e+00,  ..., -5.4127e-04,\n",
       "            7.2786e-01,  8.9352e-01],\n",
       "          [ 4.1133e-01, -4.0373e-02,  1.2075e-01,  ...,  9.1817e-01,\n",
       "           -7.8682e-01,  6.9482e-01],\n",
       "          [-1.6158e+00, -3.0775e-01,  3.6129e-01,  ..., -5.9933e-01,\n",
       "           -1.5677e-01,  8.4854e-01],\n",
       "          [-5.8981e-01, -1.7527e-01,  1.1409e+00,  ..., -1.8346e-01,\n",
       "            2.5218e-01, -6.0097e-02]],\n",
       " \n",
       "         [[-2.6271e-01, -5.5632e-02,  1.1018e+00,  ...,  8.9846e-01,\n",
       "            1.1383e-01, -1.0542e+00],\n",
       "          [ 1.4759e+00, -1.4704e-01, -4.1849e-01,  ...,  1.5485e+00,\n",
       "            2.8177e-01,  6.8216e-01],\n",
       "          [ 1.1042e+00, -7.5180e-01,  9.1925e-01,  ...,  3.9674e-01,\n",
       "            8.2830e-01, -1.5333e-01],\n",
       "          [ 2.2812e-01, -9.2426e-01, -4.1887e-01,  ..., -8.8322e-01,\n",
       "           -2.1611e+00,  2.4591e-02]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = d_model = 512 \n",
    "eps = 1e-6\n",
    "\n",
    "# 模型实例化\n",
    "ln = LayerNorm(features, eps)\n",
    "\n",
    "# 输入输出\n",
    "out_ln = ln(out_ff) # [2, 4, 512] => [batch, seq_len, d_model]\n",
    "\n",
    "out_ln.shape, out_ln "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c171f58a",
   "metadata": {},
   "source": [
    "## 4.2 正则+残差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "58a5b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "    A residual connection followed by a layer norm.\n",
    "    Note for code simplicity the norm is first as opposed to last.\n",
    "    \"\"\"\n",
    "    def __init__(self, size, dropout):\n",
    "        \"\"\"\n",
    "        :param size: 词嵌入维度\n",
    "        :param dropout: \n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.layer_norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len, d_model]\n",
    "        :param sub_mha: 实例化模型对象\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        \"Apply residual connection to any sublayer with the same size.\"\n",
    "        return x + self.dropout(sublayer(self.layer_norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "02f6ed82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[-7.3245e+00,  1.9915e+01, -2.8705e+01,  ..., -2.8111e+01,\n",
       "            1.7867e+01,  5.5224e+01],\n",
       "          [-6.6200e-01, -2.5147e+01, -3.6596e+01,  ...,  0.0000e+00,\n",
       "            2.0100e+00, -4.4888e+01],\n",
       "          [-9.4958e+00,  3.5901e+01,  1.1775e+01,  ...,  4.1708e+01,\n",
       "            7.3989e+00,  2.9574e+01],\n",
       "          [ 1.4832e+01, -8.9624e+00,  0.0000e+00,  ..., -0.0000e+00,\n",
       "            3.6257e+01, -2.8961e+01]],\n",
       " \n",
       "         [[-3.9705e+00,  8.6364e+00, -2.3460e+00,  ..., -1.5398e+01,\n",
       "           -3.8058e+01,  2.6414e+00],\n",
       "          [-1.6705e+01, -1.3782e+01,  1.7031e-01,  ...,  1.1656e+01,\n",
       "           -1.6830e+01,  2.3204e+01],\n",
       "          [-4.4710e+01,  2.4001e+01, -3.2875e+01,  ..., -3.4772e+01,\n",
       "            6.4465e+00,  1.4650e+01],\n",
       "          [-1.7462e-02,  1.5975e+01, -1.3520e+01,  ...,  2.4157e+01,\n",
       "           -3.2594e+01, -1.4680e+01]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = 8\n",
    "size = d_model = 512\n",
    "dropout = 0.2\n",
    "\n",
    "# mask = subsequent_mask(4)\n",
    "mask = None # 无掩码\n",
    "\n",
    "# 实例化mutil-head 层\n",
    "mha = MultiHeadedAttention(head, d_model)\n",
    "\n",
    "# 函数类型子层\n",
    "sub_mha = lambda x: mha(x, x, x, mask)   # Q,K,V\n",
    "\n",
    "# 模型实例化\n",
    "sc = SublayerConnection(size, dropout)\n",
    "\n",
    "# 输入输出\n",
    "out_sc = sc(out_pe, sub_mha)\n",
    "out_sc.shape, out_sc  # [2, 4, 512] => [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e837423",
   "metadata": {},
   "source": [
    "# 五、编码器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f1a01e",
   "metadata": {},
   "source": [
    "## 5.1 编码器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1568a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        :param size: 词嵌入 维度\n",
    "        :param self_attn: 多头 自注意力子层 实例化对象\n",
    "        :param feed_forward: 前馈 全连接子层 实例化对象\n",
    "        :param dropout: \n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.layer_norm = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "        print(self.layer_norm)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len, d_model]\n",
    "        :param mask: None 或 [1, seq_len, seq_len]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        x = self.layer_norm[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.layer_norm[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "8065cbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): SublayerConnection(\n",
      "    (norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (1): SublayerConnection(\n",
      "    (norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[ -6.9081,  18.1500, -29.2522,  ..., -28.0629,  18.5636,  55.2811],\n",
       "          [ -0.9034, -25.8110, -36.8332,  ...,  -0.1725,   1.2486, -45.4309],\n",
       "          [-10.1193,  35.5325,  10.8556,  ...,  41.3106,   8.7118,  29.2144],\n",
       "          [ 15.3499,  -9.7299,   0.9824,  ...,   0.1809,  37.0082, -29.8782]],\n",
       " \n",
       "         [[ -4.2244,   8.5951,  -1.1939,  ..., -15.5036, -38.4399,   2.3525],\n",
       "          [-16.5874, -13.9688,  -0.0790,  ...,  12.0542, -17.6419,  22.5610],\n",
       "          [-44.1769,  24.4744, -32.0079,  ..., -34.9427,   7.3760,  13.6904],\n",
       "          [ -0.0963,  16.8771, -12.3203,  ...,  23.8834, -31.6645, -15.1174]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 512\n",
    "head = 8\n",
    "d_model = 512\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "# mask = subsequent_mask(4) \n",
    "mask = None\n",
    "\n",
    "\n",
    "# 子层实例化\n",
    "mha = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    " \n",
    "# 模型实例化\n",
    "el = EncoderLayer(size, mha, ff, dropout)\n",
    "\n",
    "# 输入输出\n",
    "out_el = el(out_pe, mask)\n",
    "\n",
    "out_el.shape, out_el  # [2, 4, 512] => [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7871c6",
   "metadata": {},
   "source": [
    "## 5.2 编码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a33bba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"Core encoder is a stack of N layers\"\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        :param layer: 编码器层\n",
    "        :param N: 层数\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder_layers = clones(layer, N)  # 深度拷贝\n",
    "        self.layer_norm = LayerNorm(layer.size)  # d_model, size\n",
    "\n",
    "    \"Pass the input (and mask) through each layer in turn.\"\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len, d_model]\n",
    "        :param mask: [1, seq_len, seq_len]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        for layer in self.encoder_layers:\n",
    "            x = layer(x, mask)  # 输入输出\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6f0aa512",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (1): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "512 EncoderLayer(\n",
      "  (self_attn): MultiHeadedAttention(\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (feed_forward): PositionwiseFeedForward(\n",
      "    (w_1): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (w_2): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (layer_norm): ModuleList(\n",
      "    (0): SublayerConnection(\n",
      "      (layer_norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): SublayerConnection(\n",
      "      (layer_norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[-5.2359e-01,  9.0716e-01, -1.1908e+00,  ..., -1.2454e+00,\n",
       "            5.9877e-01,  1.9967e+00],\n",
       "          [ 8.9224e-02, -8.8665e-01, -1.3271e+00,  ..., -9.3186e-02,\n",
       "            1.5487e-01, -1.6146e+00],\n",
       "          [-4.5447e-01,  1.5440e+00,  4.5354e-01,  ...,  1.5994e+00,\n",
       "            4.5623e-01,  1.1132e+00],\n",
       "          [ 8.0053e-01, -6.5956e-02, -1.1237e-01,  ..., -9.6918e-02,\n",
       "            1.5565e+00, -9.3574e-01]],\n",
       " \n",
       "         [[-2.7099e-01,  5.3902e-01, -2.6727e-01,  ..., -5.6800e-01,\n",
       "           -1.5055e+00,  1.8255e-01],\n",
       "          [-7.4601e-01, -3.5349e-01,  1.5411e-03,  ...,  5.8791e-01,\n",
       "           -5.9251e-01,  7.4314e-01],\n",
       "          [-1.9300e+00,  1.0558e+00, -1.2125e+00,  ..., -1.4497e+00,\n",
       "            2.6200e-01,  5.1966e-01],\n",
       "          [-3.0690e-01,  7.6097e-01, -6.0397e-01,  ...,  1.0218e+00,\n",
       "           -1.3339e+00, -4.8538e-01]]], grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = 8\n",
    "d_model = size = 512\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "N = 8\n",
    "\n",
    "c = copy.deepcopy\n",
    "# mask = subsequent_mask(4) \n",
    "mask = None\n",
    "\n",
    "\n",
    "# 子层实例化\n",
    "mha = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "# 编码层实例化\n",
    "layer = EncoderLayer(size, c(mha), c(ff), dropout)\n",
    "print(layer.size, layer)\n",
    "\n",
    "# 编码器实例化\n",
    "en = Encoder(layer, N)\n",
    "\n",
    "# 输入输出 \n",
    "out_en = en(out_pe, mask)  # [2, 4, 512] => [batch, seq_len, d_model]\n",
    "out_en.shape, out_en"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22120261",
   "metadata": {},
   "source": [
    "# 六、解码器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c58ccf",
   "metadata": {},
   "source": [
    "## 6.1 解码器层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "fbb5b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"Decoder is made of self-attn, src-attn, and feed forward (defined below)\"\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        \"\"\"\n",
    "        :param size: 词嵌入 维度\n",
    "        :param self_attn: 多头 自注意力 实例化对象，Q=K=V\n",
    "        :param src_attn: 掩码 多头 注意力 实例化对象，Q!=K=V\n",
    "        :param feed_forward: 前馈 全连接层 实例化对象\n",
    "        :param dropout: 置0比率，让部分神经元失效\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.layer_norm = clones(SublayerConnection(size, dropout), 3)\n",
    "        print(self.layer_norm)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        :param x: 上一层的输出  [batch, seq_len, d_model]\n",
    "        :param memory: [batch, seq_len, d_model]\n",
    "        :param src_mask: [1, seq_len, seq_len]\n",
    "        :param tgt_mask: [1, seq_len, seq_len]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        m = memory  # 来自encoder 而作为 decoder的 Key-Value\n",
    "        x = self.layer_norm[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))  # 自注意力，需要掩码，防止信息泄漏\n",
    "        x = self.layer_norm[1](x, lambda x: self.src_attn(x, m, m, src_mask))  # 常规注意力，需要掩码，遮掉没有意义的注意力值\n",
    "        return self.layer_norm[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "1f8fc53b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (1): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (2): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[ -6.8838,  19.6294, -28.9570,  ..., -27.7259,  17.3384,  55.9836],\n",
       "          [  0.2049, -25.5464, -37.1251,  ...,   0.2234,   1.0651, -44.7951],\n",
       "          [ -9.3173,  35.1236,  12.3871,  ...,  40.4247,   7.6465,  29.7795],\n",
       "          [ 14.7570, -10.7893,   0.2505,  ...,  -0.0690,  36.4935, -27.8200]],\n",
       " \n",
       "         [[ -3.2919,   8.8902,  -2.3629,  ..., -15.3256, -38.0702,   2.2155],\n",
       "          [-16.0966, -14.3701,  -0.4338,  ...,  11.3824, -17.4821,  23.2981],\n",
       "          [-44.4678,  24.2947, -33.6117,  ..., -35.9683,   6.9066,  13.2562],\n",
       "          [  0.7189,  15.1474, -12.3291,  ...,  24.0046, -30.5892, -15.2597]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head = 8\n",
    "size = d_mode = 512\n",
    "d_ff = 64 \n",
    "dropout = 0.2\n",
    "\n",
    "# 模型设计\n",
    "self_attn = src_attn = MultiHeadedAttention(head, d_model, dropout)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "\n",
    "# 编码层的输出\n",
    "memory = out_en # [batch, seq_len, d_model] \n",
    "x = out_pe\n",
    "\n",
    "# decoder 掩码\n",
    "mask = subsequent_mask(4) # seq_len\n",
    "source_mask = target_mask = mask\n",
    "\n",
    "# 模型实例化\n",
    "dl = DecoderLayer(size, self_attn, src_attn, ff, dropout)\n",
    "\n",
    "# 输入输出   decoder输入“目标数据”的词嵌入表示，形式上与encoder的“源数据”的词嵌入表示相同，这里使用out_pe来充当\n",
    "out_dl = dl(x, memory, source_mask, target_mask) # [2, 4, 512] => [batch, seq_len, d_model]\n",
    "\n",
    "\n",
    "out_dl.shape, out_dl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468e76b",
   "metadata": {},
   "source": [
    "## 6.2 解码器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "006514b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"Generic N layer decoder with masking.\"\n",
    "\n",
    "    def __init__(self, layer, N):\n",
    "        \"\"\"\n",
    "        :param layer: 解码器层\n",
    "        :param N: 解码器层的个数\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder_layers = clones(layer, N)\n",
    "        self.layer_norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len, d_model]\n",
    "        :param memory: [batch, seq_len, d_model]\n",
    "        :param src_mask: [1, seq_len, seq_len]\n",
    "        :param tgt_mask: [1, seq_len, seq_len]\n",
    "        :return: [batch, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        for layer in self.decoder_layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.layer_norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "6137d93c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (1): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (2): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      ")\n",
      "512 DecoderLayer(\n",
      "  (self_attn): MultiHeadedAttention(\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (src_attn): MultiHeadedAttention(\n",
      "    (linears): ModuleList(\n",
      "      (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "      (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (feed_forward): PositionwiseFeedForward(\n",
      "    (w_1): Linear(in_features=512, out_features=64, bias=True)\n",
      "    (w_2): Linear(in_features=64, out_features=512, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (layer_norm): ModuleList(\n",
      "    (0): SublayerConnection(\n",
      "      (layer_norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): SublayerConnection(\n",
      "      (layer_norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): SublayerConnection(\n",
      "      (layer_norm): LayerNorm()\n",
      "      (dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[-0.4535,  0.3907, -1.2888,  ..., -1.3470,  0.9963,  1.6831],\n",
       "          [ 0.2102, -0.4513, -1.7127,  ...,  0.1351,  0.1781, -2.0479],\n",
       "          [ 0.1133,  1.4191,  0.2536,  ...,  1.6202,  0.3326,  1.0890],\n",
       "          [ 0.4995, -0.2464,  0.2218,  ..., -0.1123,  1.3521, -0.8444]],\n",
       " \n",
       "         [[-0.4430,  0.5039,  0.4447,  ..., -0.5091, -1.6839, -0.2979],\n",
       "          [-0.7008, -0.8361,  0.1640,  ...,  0.3541, -0.7514,  1.0254],\n",
       "          [-1.7786,  0.9805, -1.3585,  ..., -1.5161,  0.4262, -0.0173],\n",
       "          [-0.1903,  0.6061, -0.3315,  ...,  0.8285, -1.5397, -0.7765]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size = 512\n",
    "d_model = 512\n",
    "head = 8\n",
    "d_ff = 64\n",
    "dropout = 0.2\n",
    "\n",
    "c = copy.deepcopy\n",
    "\n",
    "attn = MultiHeadedAttention(head, d_model)\n",
    "ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "layer = DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout)\n",
    "print(layer.size, layer)\n",
    "\n",
    "N = 8\n",
    "\n",
    "x = out_pe\n",
    "memory = out_en  # [2, 4, 512] => [batch, seq_len, d_model]\n",
    "\n",
    "mask = subsequent_mask(4) # seq_len\n",
    "source_mask = target_mask = mask\n",
    "\n",
    "# 模型实例化\n",
    "dl = Decoder(layer, N)\n",
    "\n",
    "# 输入输出\n",
    "out_de = dl(x, memory, source_mask, target_mask)\n",
    "\n",
    "out_de.shape, out_de # [2, 4, 512] => [batch, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29773392",
   "metadata": {},
   "source": [
    "# 七、输出 linear+softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "172cc129",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \"Define standard linear + softmax generation step.\"\n",
    "    def __init__(self, d_model, vocab):\n",
    "        \"\"\"\n",
    "        :param d_model: 词嵌入维度\n",
    "        :param vocab: 词表大小\n",
    "        \"\"\"\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: [batch, seq_len, d_model]\n",
    "        :return: [batch, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        return F.log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "68f48315",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 42, 997, 486, 949],\n",
      "        [913, 348, 804,  35]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 1000]),\n",
       " tensor([[[-6.9900, -6.3757, -7.1832,  ..., -6.8251, -7.6340, -6.7575],\n",
       "          [-7.4565, -7.1753, -6.5283,  ..., -5.1829, -6.6898, -7.4696],\n",
       "          [-6.8189, -6.6836, -7.2612,  ..., -7.7652, -6.9435, -7.0307],\n",
       "          [-6.6836, -7.2830, -6.7103,  ..., -6.4768, -7.0989, -6.7283]],\n",
       " \n",
       "         [[-7.9600, -7.0918, -6.4814,  ..., -7.2976, -7.8630, -6.8312],\n",
       "          [-6.7006, -7.6107, -5.6702,  ..., -6.8366, -7.1698, -7.2947],\n",
       "          [-7.1123, -7.1748, -6.4731,  ..., -6.6584, -7.8360, -7.2967],\n",
       "          [-8.0112, -6.4194, -6.8697,  ..., -6.8192, -6.9049, -7.2557]]],\n",
       "        grad_fn=<LogSoftmaxBackward0>))"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model = 512\n",
    "vocab_size = 1000\n",
    "\n",
    "x = out_de\n",
    "\n",
    "# 模型实例化\n",
    "gen = Generator(d_model, vocab_size)\n",
    "\n",
    "# 输入输出\n",
    "out_gen = gen(x)\n",
    "\n",
    "print(torch.argmax(out_gen, dim = -1))\n",
    "out_gen.shape, out_gen # [2, 4, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b413e8b",
   "metadata": {},
   "source": [
    "# 八、编码器-解码器 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "05a97e79",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, source_embed, target_embed, generator):\n",
    "        \"\"\"\n",
    "        :param encoder: \n",
    "        :param decoder: \n",
    "        :param source_embed: 源数据嵌入\n",
    "        :param target_embed: 目标函数嵌入\n",
    "        :param generator: 类别生成器\n",
    "        \"\"\"\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        # 将参数传入到类中\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = source_embed\n",
    "        self.tgt_embed = target_embed\n",
    "        self.generator = generator\n",
    "\n",
    "    def forward(self, source, target, source_mask, target_mask):\n",
    "        \"\"\"\n",
    "        :param source: 源数据\n",
    "        :param target: 目标数据\n",
    "        :param source_mask: 对应掩码张量\n",
    "        :param target_mask: 对应掩码张量 \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return self.decode(self.encode(source, source_mask), source_mask,\n",
    "                            target, target_mask)\n",
    "\n",
    "    def encode(self, source, source_mask):\n",
    "        \"\"\"\n",
    "        :param source: \n",
    "        :param source_mask: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return self.encoder(self.src_embed(source), source_mask)\n",
    "\n",
    "    def decode(self, memory, source_mask, target, target_mask):\n",
    "        \"\"\"\n",
    "        :param memory: \n",
    "        :param source_mask: \n",
    "        :param target: \n",
    "        :param target_mask: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        return self.decoder(self.tgt_embed(target), memory, source_mask, target_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "ae447be7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 512]),\n",
       " tensor([[[-0.3687,  0.6515, -1.0932,  ..., -0.2589,  0.4345, -1.9250],\n",
       "          [ 0.2143,  1.3857, -1.0545,  ...,  0.3011,  0.4664, -2.2906],\n",
       "          [ 0.1405,  1.9436, -0.4520,  ..., -0.4401, -0.5760, -2.0684],\n",
       "          [-1.1548,  1.3899,  0.3455,  ..., -0.0339,  0.5120, -2.3459]],\n",
       " \n",
       "         [[-2.7630,  2.1301,  0.9778,  ...,  0.5441,  0.7729, -2.5819],\n",
       "          [-0.5491,  2.5325,  0.7174,  ..., -0.5978, -1.1866, -1.0995],\n",
       "          [-0.6238,  1.4725,  1.9062,  ..., -1.3275,  0.9278, -1.7078],\n",
       "          [-1.6447,  1.6734,  1.0431,  ..., -0.8960, -0.7198, -0.8516]]],\n",
       "        grad_fn=<AddBackward0>))"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 1000\n",
    "d_model = 512\n",
    "\n",
    "encoder = en\n",
    "decoder = dl\n",
    "source_embed = nn.Embedding(vocab_size, d_model)\n",
    "target_embed = nn.Embedding(vocab_size, d_model)\n",
    "generator = gen\n",
    "\n",
    "# 假设源数据与目标数据相同，实际中并不相同\n",
    "source = target = Variable(torch.LongTensor([[100, 2, 421, 508], [491, 998, 1, 221]]))\n",
    "\n",
    "# 假设src_mask 与 tgt_mask相同，实际并不相容\n",
    "source_mask = target_mask = subsequent_mask(4) # seq_len\n",
    "\n",
    "# 模型实例化\n",
    "en_de = EncoderDecoder(encoder, decoder, source_embed, target_embed, generator)\n",
    "\n",
    "# 输入输出\n",
    "out_ende = en_de(source, target, source_mask, target_mask)\n",
    "\n",
    "\n",
    "out_ende.shape, out_ende"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027c04d",
   "metadata": {},
   "source": [
    "# 九、transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "4af85eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6,\n",
    "               d_model=512, d_ff=2048, h=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    :param src_vocab: 源数据特征（词汇）总数\n",
    "    :param tgt_vocab: 目标数据特征（词汇）总数\n",
    "    :param N: 编码器和解码器堆叠数\n",
    "    :param d_model: 词嵌入维度\n",
    "    :param d_ff: 前馈全连接网络中 变换矩阵的维度\n",
    "    :param h: 头数\n",
    "    :param dropout: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = copy.deepcopy\n",
    "    \n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    \n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn),\n",
    "                             c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "\n",
    "    # This was important from their code. \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3c987b2e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModuleList(\n",
      "  (0): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (1): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (2): SublayerConnection(\n",
      "    (layer_norm): LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      ")\n",
      "EncoderDecoder(\n",
      "  (encoder): Encoder(\n",
      "    (encoder_layers): ModuleList(\n",
      "      (0): EncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): EncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (decoder_layers): ModuleList(\n",
      "      (0): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): DecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (src_attn): MultiHeadedAttention(\n",
      "          (linears): ModuleList(\n",
      "            (0): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (1): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "            (3): Linear(in_features=512, out_features=512, bias=True)\n",
      "          )\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (layer_norm): ModuleList(\n",
      "          (0): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): SublayerConnection(\n",
      "            (layer_norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "  )\n",
      "  (src_embed): Sequential(\n",
      "    (0): Embeddings(\n",
      "      (lut): Embedding(10, 512)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (tgt_embed): Sequential(\n",
      "    (0): Embeddings(\n",
      "      (lut): Embedding(10, 512)\n",
      "    )\n",
      "    (1): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Generator(\n",
      "    (proj): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "tmp_model = make_model(10, 10, 2)\n",
    "print(tmp_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "fec07ced",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%who_ls  # 显示当前所有变量名称"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
